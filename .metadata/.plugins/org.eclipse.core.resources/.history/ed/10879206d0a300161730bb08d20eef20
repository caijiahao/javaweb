#################################################  
# SVM: support vector machine  
# Author : zouxy  
# Date   : 2013-12-12  
# HomePage : http://blog.csdn.net/zouxy09  
# Email  : zouxy09@qq.com  
#################################################  

# -*- coding: utf-8 -*-  
from numpy import *
import time
import matplotlib.pyplot as plt

# calulate kernel value
def calcKernelValue(matrix_x,sampe_x,kernelOption):
   kernelType = kernelOption[0]
   numSamples = matrix_x.shape[0]
   kernelValue = mat(zeros((numSamples,1)))
   
   if kernelType == 'linear':
       kernelValue = matrix_x * sampe_x.T
   elif kernelType == 'rbf':
       sigma = kernelOption[1]
       if sigma == 0:
           sigma = 1.0
       for i in xrange(numSamples):
           diff = matrix_x[i,:] - sampe_x
           #这里T的意思是转置矩阵
           kernelValue[i] = exp(diff * diff.T / (-2.0 * sigma**2))
   else:
       raise NameError('Not support kernel type!You can use linear or rbf!')
   return kernelValue
# calculate kernel matrix given train set and kernel type 
def calcKernelMatrix(train_x,kernelOption):
    numSamples = train_x.shape[0]
    kernelMatrix = mat(zeros(numSamples,numSamples))
    for i in xrange(numSamples):
        kernelMatrix[:,i] = calcKernelValue(train_x,train_x[i, :], kernelOption)
    return kernelMatrix

# define a struct just for storing variables and data 
class SVMStruct:
    def __init__(self,dataSet,labels,C,toler,kernelOption):
        self.train_x = dataSet #each row stands for a sample
        self.train_y = labels  # corresponding label
        self.C = C             # slack variable  
        self.toler = toler     # termination condition for iterat
        self.numSamples = dataSet.shape[0]#nuber of samples
        self.alphas = mat(zeros((self.numSamples,1)))
        self.b = 0
        self.errorCache = mat(zeros((self.numSamples,2)))
        self.kernelOpt = kernelOption
        self.kernelMat = calcKernelMatrix(self.train_x,self.kernelOpt)
        
# calculate the error for alpha k 
def calcError(svm,alpha_k):
    output_k = float(multiply(svm.alphas,svm.train_y).T*svm.kernelMat[:,alpha_k]+svm.b)
    error_k = output_k - float(svm.train_y[alpha_k])
    return error_k

# update the error cache for alpha k after optimize alpha k  
def updateError(svm,alpha_k):
    error = calcError(svm,alpha_k)
    svm.errorCache[alpha_k] = [1,error]

def selectAlpha_j(svm,alpha_i,error_i):
    svm.errorCache[alpha_i] = [1,error_i]
    #numpy.nonzero表示返回矩阵内不为零的点
    candidateAlphaList = nonzero(svm.errorCache[:,0].A)[0]
    maxStep = 0;alpha_j = 0;error_j = 0;
    
    # find the alpha with max iterative step
    if len(candidateAlphaList)>1:
        for alpha_k in candidateAlphaList:
            if alpha_k == alpha_i:
                continue
            error_k = calcError(svm,alpha_k)
            if abs(error_k - error_i)>maxStep:
                maxStep = abs(error_k - error_i)
                alpha_j = alpha_k
                error_j = error_k
    
    # if came in this loop first time, we select alpha j randomly
    else:
        alpha_j = alpha_i  
        while alpha_j == alpha_i:  
            alpha_j = int(random.uniform(0, svm.numSamples))  
        error_j = calcError(svm, alpha_j) 
        
    return  alpha_j,error_j


# the inner loop for optimizing alpha i and alpha j
   def innerLoop(svm,alpha_i): 
       error_i = calcError(svm,alpha_i)
      
    
    
    
    

    
        
        
   
   
   


